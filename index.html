<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description"
    content="Refine-by-Align: Reference-Guided Artifacts Refinement through Semantic Alignment">
  <!-- <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/> -->
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="image composition, image editing, personalization, artifacts">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Refine-by-Align: Reference-Guided Artifacts Refinement through Semantic Alignment</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Refine-by-Align: Reference-Guided Artifacts Refinement through Semantic Alignment</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://song630.github.io/yizhisong.github.io/" target="_blank">Yizhi Song</a><sup>1*</sup>,</span>
                <span class="author-block">
                  <a href="https://arking1995.github.io/" target="_blank">Liu He</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://zzutk.github.io/" target="_blank">Zhifei Zhang</a><sup>2</sup>,</span>
                    <span class="author-block">
                      <a href="https://sites.google.com/view/sooyekim" target="_blank">Soo Ye Kim</a><sup>2</sup>,</span>
                      <span class="author-block">
                        <a href="https://sites.google.com/site/hezhangsprinter/" target="_blank">He Zhang</a><sup>2</sup>,</span>
                        <span class="author-block">
                          <a href="https://wxiong.me/" target="_blank">Wei Xiong</a><sup>2</sup>,</span>
                          <span class="author-block">
                            <a href="https://sites.google.com/site/zhelin625/" target="_blank">Zhe Lin</a><sup>2</sup>,</span>
                            <span class="author-block">
                              <a href="https://www.brianpricephd.com/" target="_blank">Brian Price</a><sup>2</sup>,</span>
                              <span class="author-block">
                                <a href="https://research.adobe.com/person/scott-cohen/" target="_blank">Scott Cohen</a><sup>2</sup>,</span>
                                <span class="author-block">
                                  <a href="https://jimmie33.github.io/" target="_blank">Jianming Zhang</a><sup>2</sup>,</span>
                                  <span class="author-block">
                                    <a href="https://www.cs.purdue.edu/homes/aliaga/" target="_blank">Daniel Aliaga</a><sup>1</sup></span>
                    </span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- institution -->
                    <span class="author-block">Purdue University<sup>1</sup>, Adobe<sup>2</sup><br>
                      <!-- <p style="color: rgb(238, 165, 177);">In CVPR 2024</p> -->
                    </span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper (coming soon!)</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Project (TBD)</span>
                    </a>
                  </span> -->

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (coming soon!)</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" width="150%" src="./static/images/teaser3.png">
      <h2 class="subtitle has-text-centered">
        <p style="font-family:Times New Roman">
          <b>
            Figure 1. <em>Refine-by-Align</em>. Given a generated image (with artifacts), a free-form mask indicating the artifacts region in the generated image, and a high-quality reference image containing important details such as identity logo or font, our model can automatically refine the artifacts in the generated image by leveraging the corresponding details from the reference. The proposed method could benefit various applications (e.g., DreamBooth for text-to-image customization, IDM-VTON for virtual try-on, AnyDoor for object composition, and Zero123++ for novel view synthesis).
          </b></p>
      </h2>
    </div>
  </div>
</section>
<!-- End teaser -->


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Personalized image generation has emerged from the recent advancements in generative models. However, these generated personalized images often suffer from localized artifacts such as incorrect logos, reducing fidelity and fine-grained identity details of the generated results. Furthermore, there is little prior work tackling this problem. To help improve these identity details in the personalized image generation, we introduce a new task: <em>reference-guided artifacts refinement</em>. We present <strong>Refine-by-Align</strong>, a first-of-its-kind model that employs a diffusion-based framework to address this challenge. Our model consists of two stages: <strong>Alignment Stage</strong> and <strong>Refinement Stage</strong>, which share weights of a unified neural network model. Given a generated image, a masked artifact region, and a reference image, the alignment stage identifies and extracts the corresponding regional features in the reference, which are then  used by the refinement stage to fix the artifacts. Our model-agnostic pipeline requires no test-time tuning or optimization. It automatically enhances image fidelity and reference identity in the generated image, generalizing well to existing models on various tasks including but not limited to customization, generative compositing, view synthesis, and virtual try-on. Extensive experiments and comparisons demonstrate that our pipeline greatly pushes the boundary of fine details in the image synthesis models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Pipeline -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"> The Pipeline </h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">       
            <img id="model" width="100%" src="./static/images/pipeline-v5.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 2. <em>Top</em>: During training, we train a DM for object completion, guided by a reference image. In alignment mode, the reference is a complete object, so the model learns to locate the relevant region from the reference for object completion, thus maximizing the spatial correlation in attention maps. In refinement mode, this region is directly provided as reference. <em>Bottom</em>: During inference, the inputs include a generated image with the artifacts marked, and a reference object. In the alignment stage, we perform <em>cross-attention alignment algorithm</em>  to find the correspondence map. In the refinement stage, the correspondence map is used to find the region in the reference that corresponds to artifacts, which guides refining.</b></p>
            </h3>   

        </div>
  </div>
</section>
<!-- End of pipeline -->


<!-- algorithm visualization and line charts -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"> Visualization of the Proposed Algorithm </h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">       
            <img id="model" width="100%" src="./static/images/layers-and-timesteps-v2.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman">
                <b>Figure 3. <em>Top</em>: Visualization of our cross-attention alignment algorithm. The artifacts mask is used to extract the spatial correlations between the artifacts and the reference; the output of this algorithm, the correspondence map, indicates the region in the reference that corresponds to the artifacts area.
                  <em>Middle and Bottom</em>: Correspondence maps across different transformer layers and timesteps.
                </b></p>
            </h3>   

        </div>
  </div>
</section>
<!-- End -->


<!-- algorithm -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"> Algorithm of Cross-Attention Alignment </h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">       
            <img id="model" width="70%" src="./static/images/algorithm.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman">
                <b>
                </b>Figure 4.</p>
            </h3>   

        </div>
  </div>
</section>
<!-- End -->


<!-- Ablation -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"> mIoU across Timestep and Transformer Layer </h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">       
            <img id="model" width="100%" src="./static/images/optimal_t_l.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman">
                <b>Figure 5. Running the cross-attention alignment algorithm on the test set to find the best combination of timestep and transformer layer. <em>Left:</em> mIoU across all timesteps, averaged over all layers and images; <em>Right:</em>: mIoU across all layers, averaged over all timesteps and images.
                </b></p>
            </h3>   

        </div>
  </div>
</section>
<!-- End of ablation -->

<!-- Qual -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"> Qualitative Comparisons </h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">       
            <img id="model" width="100%" src="./static/images/qual-v2.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman">
                <b>Figure 6. <strong>Qualitative comparisons</strong>. Note that the accurate reference regions corresponding to the artifacts (not the complete reference) are provided to PbE, OS and AnyDoor. In the second row of the references, we overlay the correspondence maps on them. Compared with the baselines, our model not only preserves identity (most similar to the second row), but also generate smooth and natural results where artifacts are significantly reduced.
                </b></p>
            </h3>   

        </div>
  </div>
</section>
<!-- End of Qual -->


<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/qual1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative comparison on the DreamBooth test set. Paint-by-Example and ObjectStitch lose most object details and only maintain categorical information. TF-ICON tends to copy the pose of the input subject. The comparison highlights the advantage of IMPRINT in keeping identity and making geometric changes.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/qual2.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          More qualitative comparisons.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/qual3.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          More qualitative comparisons.
       </h2>
     </div>
     <div class="item">
      <img src="static/images/qual4.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Additional qualitative comparisons with AnyDoor.
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        Coming soon!
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            </a>
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
